将指标分为满足性指标和优化指标：
    如运行时间为满足性指标，满足一个阈值即可
    而精度为优化指标，越优越好

训练集、验证集、测试级的样本分布要尽可能保持一致
    数据量少于百万级别时，建议比例是7:3或者，6：2：2
    数据量大于百万时，98:1:1也是可以的
    经验法则只是确保每个样本集都足够，能起到作用

正交化原则：
    将大的指标分解为一个个独立的互不影响的指标，分别调优

一定要确定一个可以评估算法的指标，目标不明确很危险

在算法的效果达不到人类水平时，入类可以给予指导，进步会很快
当算法的效果达到了人类水平后，人类很难用自己的经验给算法做指导。算法进步会很满。
算法的性能是有一个上限的，由数据的噪音（贝叶斯误差）决定。

假设人类误差接近贝叶斯误差，那么可以根据人类误差来计算方差引起的误差和偏差引起的误差。
    例如：
        （1）人类误差 1, 训练集误差 8， 验证集误差10
            则可避免偏差为8-1=7，方差为10-8=2，应将重点放在减少可避免偏差上
        （2）人类误差 7.5, 训练集误差 8， 验证集误差10
            则可避免偏差为8-7.5=0.5，方差为10-8=2，应将重点放在减少方差上

人类表现的水平可以用来估计贝叶斯误差

计算机擅长获取大量数据中的统计学规矩，而人类擅长自然感知任务。因此机器在基于结构化数据的任务上更容易超过人类水平。

错误分析的方法：观察模型错误分类的样本，看看各个类别的比例，从而帮助判断改善某个类别分类效果对模型整体提升的潜力。


先建立简单模型，再快速迭代

关于训练数据和测试数据样本分布有区别的情况的讨论
    如果有目标标注数据10K，有非目标标注数据（比如样本分布不用）的其他来源的数据100K。那么有三类类设置训练集、验证集、测试集的方案：
    （1）把100K与10K混合随机打乱。然后分成三个分布相同的集合。
        优点：训练集中有少量的目标数据
        缺点：验证集中大部分都不是目标数据，那么模型的目标就是错误的。
        评价：这种方法不好
    （2）把100K全部做为训练集。10K中验证集和测试集各半。
        优点：验证集的目标是正确的。
        评价：方法（2）比方法（1）好
    （3）把100K+2K的数据做为训练集。剩下的8K目标数据均分为验证集和测试集。
        优点：验证集的目标是正确的，且训练集中包含了小部分目标数据。
        评价：方法（3）最好。


-------------------------------------------------------------------------------------

卷积层需要训练的参数很少，与输入的通道有关，与输入的前几维大小无关。参数数量为（kernel的长*kernel的宽*输入数据的通道数+1）*输出数据的通道数
全连接层需要训练的参数很多，与输入数据的shape的大小有关。参数数量为（输入向量大小+1）*输出向量的大小
pooling层一般用来减少数据量

在YOLO算法中，为了处理一个格子中可能存在多目标的问题，使用了锚框机制。对于每个输出y，设定多个备用锚框，每个锚框有个标志位标志该锚框是否检测到了目标。
候选区域的算法思想：先用某种算法选出一部分的候选区域，再用另外的算法对各个候选区域进行处理


连接特定的 DNS 后缀:
描述: Realtek PCIe GbE Family Controller
物理地址: ‎4C-CC-6A-A9-C5-68
已启用 DHCP: 是
IPv4 地址: 10.12.42.156
IPv4 子网掩码: 255.255.255.0
获得租约的时间: 2020年10月10日 20:04:54
租约过期的时间: 2020年10月11日 0:04:53
IPv4 默认网关: 10.12.42.2
IPv4 DHCP 服务器: 172.17.100.21
IPv4 DNS 服务器: 172.16.7.10, 172.16.7.30
IPv4 WINS 服务器:
已启用 NetBIOS over Tcpip: 是
连接-本地 IPv6 地址: fe80::8cb1:b393:c3d:a2bc%8
IPv6 默认网关:
IPv6 DNS 服务器: